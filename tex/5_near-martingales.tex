% !TeX root = ../dissertation.tex

\section{Motivation}
\footnotetextonly{Parts of this chapter previously appeared in the following open-access article: \fullcite{KuoShresthaSinhaSundar2022}.}

We now come to a central motivation behind the definition of the Ayed–Kuo integral --- martingales. Recall that a martingale \( M \) is an integrable adapted process such that \( \E\br{M_t \given ℱ_s} = M_s \) almost surely for every \( s ≤ t \) (see \cref{sec:martingales}). Moreover, the process \( M_t = ∫_a^t X_s \dif W_s \) generated by the Itô's integral of an adapted process \( X_s \) is a martingale. Can we say the same about processes associated with Ayed–Kuo integrals?

\begin{example}  \label{eg:integral_Wb_t_linearity}
    Consider the process \( N(t) = ∫_a^t W_b \dif W_t \). Using the linearity of the integral and \cref{eg:integral_Wb}, we can write
    \begin{align*}
        N(t)
        & =  ∫_a^t W_b \dif W_s  \\
        & =  ∫_a^b W_b \dif W_s - ∫_t^b W_b \dif W_s  \\
        & =  [W_b (W_b - W_a) - (b - a)] - [W_b (W_b - W_t) - (b - t)]  \\
        & =  W_b (W_t - W_a) - (t - a) .
    \end{align*}
    We write \( W_b = (W_b - W_t) + (W_t - W_s) + W_s \) and \( W_t - W_a = (W_t - W_s) + (W_s - W_a) \). Using the independence of increments of Wiener process, we get
    \begin{equation*}
        \E\br{N(t) \given ℱ_s}  =  W_s (W_s - W_a) - (s - a)  ≠  N(s) .
    \end{equation*}
    So the process \( N \) is not a martingale. However, note that \( \E\br{N(s) \given ℱ_s} = W_s (W_s - W_a) - (s - a) = \E\br{N(t) \given ℱ_s} \), or equivalently, \( \E\br{N(t) - N(s) \given ℱ_s} = 0 \).
\end{example}

The above example motivates the following definition.
\begin{definition}[{\cite[definition 2.1]{HwangKuoSaitôZhai2017}}]  \index{near-martingale}  \index{near-submartingale}  \index{near-supermartingale}
    An integrable stochastic process \( N \) is called a \emph{near-martingale} if \( \E\br{N(t) - N(s) \given ℱ_s} = 0 \) almost surely for every \( s ≤ t \). It is called a \emph{near-submartingale} if we replace the second condition with \( \E\br{N(t) - N(s) \given ℱ_s} ≥ 0 \), and is called a \emph{near-supermartingale} when \( \E\br{N(t) - N(s) \given ℱ_s} ≤ 0 \).
\end{definition}
Clearly, a process that is both a near-submartingale and a near-supermartingale is a near-martingale. Moreover, any result that is true for a near-submartingale can be suitably modified for a near-supermartingale, and subsequently for a near-martingale. Therefore, in what follows, we only show results for submartingales.

For any near-submartingale \( N \), we will called the process \( M_t = \E\br{N(t) \given ℱ_t} \) as the \emph{conditioned process}\index{conditioned process}.

It is evident from the definition that every submartingale is a near-submartingale. More generally, a near-submartingale and its conditioned process are related by the following fundamental result.
\begin{proposition}[{\cite[theorem 2.5]{HwangKuoSaitôZhai2017}}]  \label{thm:near-martingale_martingale}
    A process \( N \) is a near-submartingale if and only if the conditioned process \( M \) given by \( M_t = \E\br{N(t) \given ℱ_t} \) is a submartingale.
\end{proposition}
\begin{proof}
    Note that the integrability condition is trivially satisfied. We prove the result for the \emph{sub}-case. The other cases can be similarly derived. In what follows, we assume \( s ≤ t \) and hence \( ℱ_s ⊆ ℱ_t \).

    For one direction, assume that \( N \) is a near-submartingale. Then
    \begin{equation*}
        \E\br{M_t \given ℱ_s}
        =  \E\br{\E\br{N(t) \given ℱ_t} \given ℱ_s}
        =  \E\br{N(t) \given ℱ_s}
        ≥  \E\br{N(s) \given ℱ_s}
        =  M_s ,
    \end{equation*}
    so \( M \) is a submartingale.

    For the other direction, assume \( M \) is a submartingale. Then
    \begin{equation*}
        \E\br{N(t) \given ℱ_s}
        =  \E\br{\E\br{N(t) \given ℱ_t} \given ℱ_s}
        =  \E\br{M_t \given ℱ_s}
        ≥  M_s
        =  \E\br{N(s) \given ℱ_s} ,
    \end{equation*}
    so \( N \) is a near-submartingale.
\end{proof}



\section{Ayed–Kuo integrals are near-martingales}  \label{sec:Ayed–Kuo_near-martingale}

The following theorem shows that Ayed–Kuo integrals are near-martingales.

\begin{theorem}[{\cite[theorem 3.3]{KuoShresthaSinhaSundar2022}}]  \label{thm:Ayed–Kuo_integral_near-martingale}
    Suppose \( Θ \) is a real-valued measurable function on \( ℝ^2 \). Assume \( L^2(Ω) \)-convergence for the Ayed–Kuo integral. Then \( N(t) = ∫_a^t Θ(W_u, W_b - W_u) \dif W_u \) is a near-martingale.
\end{theorem}
\begin{proof}
    The definition of the Ayed–Kuo integral under \( L^2(Ω) \)-limits implies that for any \( s ≤ t \),
    \begin{align*}
        \E\bs{N(t) - N(s) \given ℱ_s}
        & =  \E\bs{∫_s^t Θ(W_u, W_b - W_u) \dif W_u \given ℱ_s}  \\
        & =  \E\bs{\lim_{n → ∞} ∑_{i = 1}^∞ Θ(W_{t_{i-1}}, W_b - W_{t_i}) \Del W_i \given ℱ_s}  \\
        & =  \lim_{n → ∞} ∑_{i = 1}^∞ \E\bs{Θ(W_{t_{i-1}}, W_b - W_{t_i}) \Del W_i \given ℱ_s} .
    \end{align*}

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}
        \centering
            %  Background squares

            % ℋ_{t_{i-1}}^{t_i}
            \fill [Brown!12.5] (0,0) rectangle (3,2);
            \fill [Brown!12.5] (5,0) rectangle (8,2);
            \node (separation) at (4, 2.75) [Brown] {\( ℋ_{t_{i-1}}^{t_i} \)};
            \draw [->, thick, color=Brown] (separation.west) to [bend right=10] (1.5,1.75);
            \draw [->, thick, color=Brown] (separation.east) to [bend left=10 ] (5.5,1.75);
            
            % ℱ_s
            \node (separation) at (0, 2.5) [ForestGreen] {\( ℱ_s \)};
            \draw [-latex, thick, color=ForestGreen] (separation.east) to [bend left=10] (0.5,1.75);
            \fill [pattern=north west lines, pattern color=ForestGreen] (0,0) rectangle (1,2);

            % Main timeline
            \draw [->, very thick, color=gray] (-2,0) -- (9,0) node[below] {\( t \)};
            \foreach \x/\xtext in {-1/0, 0/a, 1/s, 3/t_{i-1}, 5/t_i, 7/t, 8/b}
                \draw [ultra thick] (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {$\xtext$};

            % Left process
            \draw [very thick, color=Brown] (0,0.75) -- (3,0.75);
            \draw [color=Brown] (2,1.1) node {\( W_{t_{i-1}} \)};
            \draw [fill=Brown] (0,0.75) circle (1.5pt);
            \draw [fill=Brown] (3,0.75) circle (1.5pt);

            % Right process
            \draw [very thick, color=Brown] (5,0.75) -- (8,0.75);
            \draw [color=Brown] (6.5,1) node {\( W_1 - W_{t_i} \)};
            \draw [fill=Brown] (5,0.75) circle (1.5pt);
            \draw [fill=Brown] (8,0.75) circle (1.5pt);

            % Center process
            \draw [very thick, color=SteelBlue] (3,1.25) -- (5,1.25);
            \draw [color=SteelBlue] (4,1.6) node {\( W_{t_i} - W_{t_{i-1}} \)};
            \draw [fill=SteelBlue] (3,1.25) circle (1.5pt);
            \draw [fill=SteelBlue] (5,1.25) circle (1.5pt);

            % Setup and bounding box
            % \clip(-2,-1) rectangle (9,3);
            % \draw (current bounding box.north east) rectangle (current bounding box.south west);
        \end{tikzpicture}
        \caption{Inclusion of σ-algebras along with disjoint increments of \( W \).}
        \label{fig:σ-algebra_inclusion}
    \end{figure}

    Using the tower property of conditional expectation (\cref{thm:conditional_expectation_properties}) and the inclusion \( ℋ_{t_{i-1}}^{t_i} ⊇ ℱ_s \) (see \cref{fig:σ-algebra_inclusion}), we get
    \begin{equation*}
        \E\bs{ Θ(W_{t_{i-1}}, W_b - W_{t_i}) \Del W_i \given ℱ_s }
        =  \E\bs{ \E\br{ Θ(W_{t_{i-1}}, W_b - W_{t_i}) \Del W_i \given ℋ_{t_{i-1}}^{t_i} } \given ℱ_s } .
    \end{equation*}
    Since \( Θ(W_{t_{i-1}}, W_b - W_{t_i}) \) is \( ℋ_{t_{i-1}}^{t_i} \)-measurable and \( \Del W_i \) is independent to \( ℋ_{t_{i-1}}^{t_i} \), we get
    \begin{equation*}
        \E\bs{ Θ(W_{t_{i-1}}, W_b - W_{t_i}) \Del W_i \given ℱ_s }
        =  \E\bs{Θ(W_{t_{i-1}}, W_b - W_{t_i}) \cancelto{0}{\E\br{\Del W_i}} \given ℱ_s }
        =  0 .
    \end{equation*}
    By the continuity of limits, we get our desired result \( \E\bs{N(t) - N(s) \given ℱ_s} = 0 \).
\end{proof}

\begin{remark}
    Note that the above proof also implies \( \E\bs{N(t) - N(s) \given G^t} = 0 \) for any \( s ≤ t \). This shows that Ayed–Kuo integrals are also \emph{backward near-martingales}\index{near-martingale!backward}. Similarly, the process \( \widetilde{N}(t) = ∫_t^b Θ(W_u, W_b - W_u) \dif W_u \) is a near-martingale and a backward near-martingales by the same logic.
\end{remark}

Let us look at an example of the above proposition.
\begin{example}[{\cite[example 2.7]{HwangKuoSaitô2019}}]
    In \cref{eg:integral_Ws_W_b-W_s}, we showed that
    \begin{equation*}
        Z(t) = ∫_a^t W_s (W_b - W_s) \dif W_s  =  \frac12 W_b \br{(W_t^2 - W_a^2) - (t - a)} - \frac13 \br{W_t^3 - W_a^3} .
    \end{equation*}
    From \cref{thm:Ayed–Kuo_integral_near-martingale}, we conclude that \( Z(t) \) is a near-martingale. On the other hand, we can verify this using the conditional expectation \( M_t = \E\br{Z(t) \given ℱ_t} \) and \cref{thm:near-martingale_martingale}. Now,
    \begin{align*}
        M_t
        & =  \E\br{Z(t) \given ℱ_t}  \\
        & =  \frac12 \E\br{W_b \given ℱ_t} \br{(W_t^2 - W_a^2) - (t - a)} - \frac13 \br{W_t^3 - W_a^3}  \\
        & =  \frac12 W_t \br{(W_t^2 - W_a^2) - (t - a)} - \frac13 \br{W_t^3 - W_a^3}  \\
        & =  \frac16 \br{(W_t^3 - 3 t W_t) - 3 (W_a^2 - a) W_t + 2 W_a^2} .
    \end{align*}
    We can easily check that \( W_t \) and \( W_t^3 - 3 t W_t \) are martingales. Therefore \( M \) is also a martingale, as expected.
\end{example}

The product of a martingale and an instantly-independent process is a near-martingale if and only if the instantly-independent process has constant mean.
\begin{proposition}[{\cite[theorem 2.9]{HwangKuoSaitô2019}}]  \label{thm:martingale_instantlyindependent_product}
    Suppose \( M \) is a submartingale and \( Y \) an instantly-independent process such that both \( M_t \) and \( Y^t \) are square-integrable for each \( t \). Then the process \( N \) given by \( N(t) = M_t Y^t \) is a near-submartingale if and only if \( \E\br{Y^t} \) is a constant.
\end{proposition}
\begin{proof}
    Note that
    \begin{equation*}
        \E\br{N(t) \given ℱ_t}
        =  \E\br{M_t Y^t \given ℱ_t}
        =  M_t \E\br{Y^t \given ℱ_t}
        =  M_t \E\br{Y^t} .
    \end{equation*}
    Therefore, \( \E\br{N(t) \given ℱ_t} \) is a submartingale if and only if \( \E\br{Y^t} \) is a constant. By \cref{thm:near-martingale_martingale}, \( N(t) \) is a near-submartingale if and only if \( \E\br{Y^t} \) is a constant.
\end{proof}



\section{Stopped near-martingales}

In this section, we show that stopped near-martingales are near-martingales. Moreover, we give a optional stopping theorem for near-martingales on the lines of Doob's optional stopping theorem (\cref{thm:optional_stopping_Doob}).

\begin{definition}  \label{def:near-martingale_transform}
    Let \( \br{A_n}_{n = 0}^∞ \) be an adapted process and \( \br{X_n}_{n = 0}^∞ \) a discrete time near-submartingale. Then the processes \( \br{Y_n}_{n = 0}^∞ \), where \( Y_0 = 0 \) and
    \begin{equation*}
        Y_n = (A ∙ X)_n = ∑_{i = 1}^n A_{n-1} (X_n - X_{n-1})
    \end{equation*}
    is called the \emph{near-martingale transform}\index{near-martingale transform} of \( X \) by \( A \).
\end{definition}

Near-martingale transforms retain the near-martingale property. This is a generalization \cref{thm:martingale_transform} to near-martingales.
\begin{proposition}  \label{thm:near-martingale_transform}
    \begin{enumerate}
        \item \label{itm:near-martingale_transform_bounded_positive}  If \( X \) is a near-submartingale and \( A \) is a bounded non-negative adapted process, then \( (A ∙ X) \) is a near-submartingale.
        \item \label{itm:near-martingale_transform_bounded} If \( X \) is a near-martingale and \( A \) is a bounded adapted process, then \( (A ∙ X) \) is a near-martingale.
        \item  If \( X \) and \( A \) are both square integrable, then we do not require the boundedness condition in \cref{itm:near-martingale_transform_bounded_positive,itm:near-martingale_transform_bounded}.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We only prove \cref{itm:near-martingale_transform_bounded_positive} because the rest follow the same process. Let \( X \) be a near-submartingale and \( Y = (A ∙ X) \). Suppose \( n \) is an arbitrary time. Note that \( Y_n - Y_{n-1} = A_{n-1} (X_n - X_{n-1}) \), which is integrable since \( A \) is bounded. Using the adaptedness of \( A \), we get
    \begin{equation*}
        \E\br{Y_n - Y_{n-1} \given ℱ_{n-1}}
        =  \E\br{A_{n-1} (X_n - X_{n-1}) \given ℱ_{n-1}}
        =  A_{n-1} \E\br{X_n - X_{n-1} \given ℱ_{n-1}}
        ≥  0 ,
    \end{equation*}
    where the last inequality holds since \( A \) is non-negative.
\end{proof}

The following result says that stopped near-submartingales are near-submartingales.
\begin{theorem}  \label{thm:near-martingale_stopped}
    Suppose \( X \) is a discrete time near-submartingale and \( τ \) a stopping time. Then the stopped process \( X^τ \) defined by \( X^τ_n = X_{τ ∧ n} \) is a (discrete time) near-submartingale.
\end{theorem}
\begin{proof}
    Let \( A_n = 𝟙_{\bc{n ≤ τ}} \). Clearly, the process \( A \) is bounded, non-negative, and adapted. Now, note that \( X^τ_n - X_0 = X_{τ ∧ n} - X_0 = (A ∙ X)_n \). Therefore, by \cref{thm:near-martingale_transform}, we get that \( X^τ \) is a near-submartingale.
\end{proof}



Now, we show the equivalent result of Doob's optional stopping theorem (\cref{thm:optional_stopping_Doob}) for discrete time near-submartingales\index{optional stopping theorem!near-martingale}.
\begin{theorem}  \label{thm:optional_stopping_near-martingale_discrete_time}
    Let \( X \) be a discrete time near-submartingale. Suppose \( σ \) and \( τ \) are two bounded stopping times with \( σ ≤ τ \). Then \( X_σ \) and \( X_τ \) are integrable, and \( \E\br{X_τ - X_σ \given ℱ_σ} ≥ 0 \) almost surely.
\end{theorem}
\begin{proof}
    Since \( σ \) and \( τ \) are bounded, there exists \( N < ∞ \) such that \( σ ≤ τ ≤ N \). Let \( Y \) be any near-submartingale. Clearly, \( Y_σ \) is integrable. Suppose \( E ∈ ℱ_σ \). Then for any \( n ≤ N \), we have \( E ∩ \bc{σ = n} ∈ ℱ_n \), and so
    \[ ∫_{E ∩ \bc{σ = n}} \br{Y_N - Y_σ} \dif \Pr  ~ =  ∫_{E ∩ \bc{σ = n}} \br{Y_N - Y_n} \dif \Pr  ~ ≥  0 . \]
    Summing over \( n \), we get \( ∫_E \br{Y_N - Y_σ} \dif \Pr ≥ 0 \), and so \( \E\br{Y_N - Y_σ \given ℱ_σ} ≥ 0 \). Finally, let \( Y_n = X^τ_n \) to get
    \[ \E\br{X^τ_N - X^τ_σ \given ℱ_σ}  =  \E\br{X_τ - X_σ \given ℱ_σ}  ≥  0 . \]
\end{proof}

We need the following definition and lemma to prove the result in continuous time.
\begin{definition}
    Let \( \br{ℱ_n}_{n = 1}^∞ \) be a decreasing sequence of σ-algebras, and let \( X = \br{X_n}_{n = 1}^∞ \) be a stochastic process. Then the pair \( \br{X_n, ℱ_n}_{n = 1}^∞ \) is called a \emph{backward near-submartingale} if for every \( n \),
    \begin{enumerate}
        \item  \( X_n \) is integrable and \( ℱ_n \)-measurable, and
        \item  \( \E\br{X_n - X_{n+1} \given ℱ_{n+1}} ≥ 0 \).
    \end{enumerate}
\end{definition}

\begin{lemma}  \label{thm:backward_near-submartingale_UI}
    Let \( \br{X_n, ℱ_n}_{n = 1}^∞ \) be a backward near-submartingale with \( \E\br{X_n} > -∞ \). If \( X \) is non-negative for every \( n \), then \( X \) is uniformly integrable.
\end{lemma}
\begin{proof}
    As \( n ↗ ∞ \), we have \( \E\br{X_n} ↘ \lim_{n → ∞} \E\br{X_n} = \inf_n \E\br{X_n} > -∞ \). Fix \( ϵ > 0 \). By the definition of infimum, there exists a \( N > 0 \) such that for any \( n ≥ N \), we have \( \E\br{X_N} - \lim_{n → ∞} \E\br{X_n} < ϵ \).

    For any \( k ≥ n \) and \( δ > 0 \), we have
    \begin{equation*}
        \E\br{\abs{X_k} 𝟙_{\bc{\abs{X_k} > δ}}}
        =  \E\br{X_k 𝟙_{\bc{X_k > δ}}} + \E\br{ X_k 𝟙_{\bc{X_k ≥ -δ}}} - \E\br{X_k} .
    \end{equation*}
    Moreover, since \( X \) is a backward near-submartingale, \( \E\br{X_k 𝟙_{\bc{X_k ≥ δ}}} ≤ \E\br{X_n 𝟙_{\bc{X_k ≥ δ}}} \). Therefore,
    \begin{align*}
        \E\br{\abs{X_k} 𝟙_{\bc{\abs{X_k} > δ}}}
        & ≤  \E\br{X_n 𝟙_{\bc{X_k > δ}}} + \E\br{X_n 𝟙_{\bc{X_k ≥ -δ}}} - \br{\E\br{X_n} - ϵ}  \\
        & ≤  \E\br{\abs{X_n} 𝟙_{\bc{\abs{X_k} > δ}}} + ϵ .
    \end{align*}
    By Markov's inequality and the non-negativity of \( X \),
    \begin{equation*}
        \Pr\bc{\abs{X_k} > δ}
        ≤  \frac1δ \E\abs{X_k}
        =  \frac1δ \E\br{X_k}
        ≤  \frac1δ \E\br{X_0}
        → 0
    \end{equation*}
    as \( δ → ∞ \). This concludes the proof.
\end{proof}

We are now ready to prove the near-martingale optional stopping theorem in continuous time.
\begin{theorem}  \label{thm:optional_stopping_near-martingale_continuous}
    Let \( N \) be a near-submartingale with right-continuous sample paths. Suppose \( σ \) and \( τ \) are two bounded stopping times with \( σ ≤ τ \). If \( N \) is non-negative or uniformly integrable, then \( N(σ) \) and \( N(τ) \) are integrable, and \[ \E\br{N(τ) - N(σ) \given ℱ_σ} ≥ 0  \text{ almost surely.} \]
\end{theorem}
\begin{proof}
    We use a discretization argument to prove the result. Let \( T > 0 \) be a bound for \( τ \). For every \( n ∈ ℕ \), define the discretization function
    \begin{equation}
        f_n: [0, ∞) → \bc{\frac{k}{n} : k = 0, \dotsc, n}: x ↦ \frac{\floor{2^n x} + 1}{2^n} ∧ T ,
    \end{equation}
    and let \( σ_n = f_n(σ) \) and \( τ_n = f_n(τ) \).
    
    Now, for any \( n \) and \( t \),
    \begin{equation*}
        \bc{τ_n ≤ t}
        =  \bc{f_n(τ) ∈ [0, t]}
        =  \bc{τ ∈ f_n^{-1}[0, t]}
        =  \bc{τ ∈ f_n^{-1} \bs{0, {\frac{\floor{2^n t}}{2^n}}}}
        ∈  ℱ_{\frac{\floor{2^n t}}{2^n}} ⊆ ℱ_t ,
    \end{equation*}
    so \( τ_n \) is a stopping time. Similarly, \( σ_n \) is a stopping time. Moreover, it can be easily seen that \( σ_n ≤ τ_n \) for every \( n \), and \( σ_n ↘ σ \) and \( τ_n ↘ τ \) as \( n ↗ ∞ \). Therefore, by the discrete time near-submartingale optional stopping theorem \cref{thm:optional_stopping_near-martingale_discrete_time}, we get \( N(σ_n) \) and \( N(τ_n) \) are integrable, and \( \E\br{N(τ_n) - N(σ_n) \given ℱ_{σ_n}} ≥ 0 \) almost surely. Furthermore, it is easy to see that \( ℱ_σ = ⋂_{n = 1}^∞ ℱ_{σ_n} ⊆ ℱ_{σ_n} \) for any \( n \). Therefore, \( \E\br{N(τ_n) - N(σ_n) \given ℱ_σ} ≥ 0 \) almost surely.

    If \( N \) is non-negative, by construction, \( \br{N_{σ_n}, ℱ_{σ_n}}_{n = 1}^∞ \) is a backward near-submartingale such that \( N_{σ_n} ≥ 0 \) for every \( n \). Therefore, \( \E\br{N(σ_n)} ↘ \E\br{N(σ)} > -∞ \) as \( n ↗ ∞ \). Using \cref{thm:backward_near-submartingale_UI}, \( \br{N(σ_n)}_{n = 1}^∞ \) is uniformly integrable. Similarly, \( \br{N(τ_n)}_{n = 1}^∞ \) is also uniformly integrable. On the other hand, if \( N \) is uniformly integrable, this is trivial.

    Using the right continuity of \( N \) and the boundedness assumption of \( σ \) and \( τ \), we get \( \lim_{n → ∞} N(σ_n) = N(σ) \) and \( \lim_{n → ∞} N(τ_n) = N(τ) \) almost surely. Furthermore, the uniform integrability of \( \br{N(σ_n)}_{n = 1}^∞ \) and \( \br{N(τ_n)}_{n = 1}^∞ \) allows us to conclude that \( N(σ) \) and \( N(τ) \) are integrable and that the convergence is also in \( L^1 \), giving us \( \E\br{N(τ) - N(σ) \given ℱ_σ} ≥ 0 \) almost surely.
\end{proof}

We highlight the special case of \cref{thm:optional_stopping_near-martingale_continuous}.
\begin{corollary}  \label{thm:optional_stopping_near-martingale_special}
    Let \( N \) be a non-negative near-martingale with right-continuous sample paths and \( τ \) a bounded stopping time. Then \( N(τ) \) is integrable, and \( \E\br{N(τ)} = \E\br{N(0)} \) almost surely.
\end{corollary}
